==============================================================================
The Net2 Protocol Benchmark
==============================================================================

:TEP: 140
:Group: Network Working Group 
:Type: Informational
:Status: Draft
:TinyOS-Version: > 2.1
:Author: Thanh Dang, Kaisen Lin, Chieh-Jan Mike Liang, and Omprakash Gnawali

:Draft-Created: 12-Jul-2010
:Draft-Version: $Revision: 1.1 $
:Draft-Modified: $Date: 2010-07-12 22:40:39 $
:Draft-Discuss: TinyOS Developer List <tinyos-devel at mail.millennium.berkeley.edu>

.. Note::

   This memo documents a part of TinyOS for the TinyOS Community, and
   requests discussion and suggestions for improvements.  Distribution
   of this memo is unlimited. This memo is in full compliance with
   TEP 1.

Abstract
==============================================================================

The memo describes the metrics, test scenarios, test test
applications, and analysis tools for testing collection and
dissemination protocols in TinyOS 2.x. Our goal is to design
benchmarks for network protocols.


1. Introduction
==============================================================================

TinyOS 2.x comes with a number of collection and dissemination
protocols. Collection is a network-layer best-effort protocol that
forms routes from all the nodes in a network to the collection roots
or sinks [1]_. CTP [2]_ and MultihopLQI collection protocols are available
in TinyOS 2.x. Dissemination is a network-layer reliable protocol that
allows dissemination of key-value pairs to the entire network
[3]_. Drip, DIP, and DHV dissemination protocols are available in
TinyOS 2.x. Although a large number of mature protocols are available,
we lack standard performance tests to evaluate these protocols.

In this document, we describe benchmarks that allow us to
systematically compare the performance of these protocols. Benchmarks
also enable us to compare improvements to these protocols using a
standard set of test cases, scenarios, and tools. We hope the
community will adopt these benchmarks so that the results from
different research projects can be more directly compared.


2. The Net2 Protocol Benchmark Overview
==============================================================================

There are two components in this benchmark. First, a description of a
series of tests to run to evaluate the protocols. We describe the
topologies in which to test the protocols, the protocol use scenarios,
and protocol parameters to vary across the experiments. The second
component of the net2 protocol benchmark is the metrics that describe
the performance of the protocols when we run them on the series of
scenarios included in the benchmark.


3. Topologies
==============================================================================

To make sure the experiments can be repeatable, the network topologies 
should remain the same when the experiments are repeated for different 
protocols. Due to the dynamic nature of testbeds, it is difficult to
repeat an exact experiment on testbeds. We therefore focus on only 
specifying topologies for simulations in this benchmark.


Topologies used in simulations can be created by either duplicating topologies 
of testbeds or by artificial generation. For testbed topologies, there are 
available topologies from  Motelab, Mirage,...?, which can be accessible at
urls..?. For artificial topologies, experiments SHOULD use the following topologies.

a) Single-hop star topology
b) Single-hop clique topology
c) Multi-hop chain tolology
d) Multi-hop grid topology
e) Multi-hop random topology
                                              
               ____                  __ __
 \ | /        /\  /\                |  |  |
__\|/__      /__\/__\     _ _ _ _   |__|__|          
  /|\        \  /\  /               |  |  | 
 / | \        \/__\/                |__|__| 
  a)            b)          c)        d)                  e)

The network size (total number of nodes) and the network density (average number
of neighbors) can be varied but should remain the same for the same set of 
experiments.

The network dynamic, which is defined by the rate at which nodes join and leave
a network (eg. unreliable or mobile sensor networks) should also remain the same
for the same set of experiments.


4. Protocol Use Scenarios
==============================================================================

The following scenarios SHOULD be considered in evaluating dissemination 
protocols:

Scenario 1: A node disseminates one or more new items to a stable network.
This scenario occurs in practice when a node or a basestation reprograms a 
network or disseminates a events to all nodes in a network.

Scenario 2: A node joins an updated network. This scenario occurs in
practice when one or more items have been disseminated to the network. 
However, there are new nodes (eg. mobile nodes moving into the region,
or intermitent nodes that were off during the dissemination) join 
the network.

Scenario 3: Multiple nodes joining and leaving a network at a specified rate.
This scenario occurs in practice where mobile nodes moving in and out a
region under dissemination.

Scenario 4: Multiple items with different versions are injected into
the network from different nodes. This scenario occurs in practice where 
multiple users accessing a shared sensing network and commanding the
network to perform different tasks at the same time. 


The following scenarios SHOULD be considered in evaluating collection 
protocols:

5. Protocol Parameters
==============================================================================

There are a number of parameters affecting the performance of collection
and dissemination protocols. Protocol performance MUST be described with 
the parameters' values.


Link layer 
   + Link quality (TOSSIM only) 
   + LPL/not LPL

Network layer 
   + Trickle Suppression constant (if protocols is trickle-based)

Application layer
   + Item size (within a TOS message for now)
   + Total number of items
   + Total number of new items


The following are meaningful values for each paraters
 + (20) Link layer   
	- (10) link quality
        - (2)  LPL/not LPL

 + (3) Network layer
	- (3) suppression constant : 1,2,3

 + (450) Application layer
	- (2) Item size                  : 2, 10 (Bytes)
        - (15) Total number of items     : 8, 16,..., 128
        - (15) Total number of new items : 8, 16,..., 128
 
 + (7) Network topologies
	- (2) Single-hop : star, clique
        - (5) Multi-hop  : chain, grid, star-chain, random, real-trace

 + (3) Network desity
	- (3) Network density : sparse, medium, dense
 
 + (5) Network dynamic
	- joining/leaving rate : 1%, 5%, 10%, 15%, 20%


6. Running the Benchmark
==============================================================================


Total number of test cases : 4*20*3*450*7*3*5 = 8505000

Each test case should be repeated 10 times. Total number of experiments are 85050000


Note: Experimental design techniques can be used to reduce the number of experiments 
      depending on what metrics are being compared. 


Generic testing application to cover all test cases are to be developed
and released (in apps/tests?)


7. Metrics
==============================================================================

There are different metrics for collection and dissemination protcols.

Total number of transmitted messages

Updating latency for a network

Updating latency for a node

Fraction of the network that is updated

Energy consumption 




8. Result Analysis Tools
==============================================================================

Analysis scripts (eg. python, shell, matlab) are to be developed and
released (in tinyos-2.x/support?)


9. Authors
====================================================================

| Thanh Dang
| PSU
| email - dangtx@pdx.edu
|
| Kaisen Lin
| UCSD
| email - kaisenl@cs.ucsd.edu
|
| Chieh-Jan Mike Liang
| 213 NEB, 3400 N Charles St
| Johns Hopkins University
| Baltimore, MD 21211
|
| email - cliang4@cs.jhu.edu
|
| Omprakash Gnawali
| S255 Clark Center, 318 Campus Drive
| Stanford University
| Stanford, CA  94305
|
| phone - +1 650 725 6086
| email - gnawali@cs.stanford.edu


10. Citations
====================================================================

.. [1] TEP 119: Collection

.. [2] TEP 123: The Collection Tree Protocol (CTP) 

.. [2] TEP 118: Dissemination of Small Values
